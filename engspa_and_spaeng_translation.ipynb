{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aishanisingh/LiveConv/blob/main/engspa_and_spaeng_translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Translator**: Spanish-to-English & English-to-Spanish translation build using Transformer Architecture"
      ],
      "metadata": {
        "id": "dx85PNfMxzWa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "fy9gILLYwVSz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os,io\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import tensorflow as tf\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Some constants here\n",
        "# This variable determines whether the model learns Spanish-to-English translation or English-to-Spanish translation.\n",
        "# Once the epochs are finished, the weights are stored in Google Drive. During inference,\n",
        "# the weights corresponding to the detected input language are loaded.\n",
        "\n",
        "TRANSLATION_PATH=\"engspa\"\n",
        "#TANSLATION_PATH=\"spaeng\"\n",
        "EPOCHS=10"
      ],
      "metadata": {
        "id": "sD-3ceHXezzW"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading the file\n",
        "zip_file = tf.keras.utils.get_file(\n",
        "    'spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
        "    extract=True)\n",
        "\n",
        "file_path = os.path.dirname(zip_file)+\"/spa-eng/spa.txt\""
      ],
      "metadata": {
        "id": "Xtpf_17ZMiv7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58a708df-46a4-481c-fdcc-a3e9b67b19f1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
            "2638744/2638744 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! head -10 /root/.keras/datasets/spa-eng/spa.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68ctCwgzMmPE",
        "outputId": "b5438a24-6c78-49a2-ffff-9e5668330e28"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Go.\tVe.\n",
            "Go.\tVete.\n",
            "Go.\tVaya.\n",
            "Go.\tVáyase.\n",
            "Hi.\tHola.\n",
            "Run!\t¡Corre!\n",
            "Run.\tCorred.\n",
            "Who?\t¿Quién?\n",
            "Fire!\t¡Fuego!\n",
            "Fire!\t¡Incendio!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(path, size=None):\n",
        "    text = io.open(file_path, encoding='UTF-8').read()\n",
        "    lines = text.splitlines()\n",
        "    pairs = [line.split('\\t') for line in lines]\n",
        "\n",
        "    if TRANSLATION_PATH == \"spaeng\":\n",
        "      source = np.array([source for target, source in pairs])  # extract source text into a numpy array\n",
        "      target = np.array([target for target, source in pairs])  # extract target text into a numpy array\n",
        "    else:\n",
        "      source = np.array([source for source, target in pairs])  # extract source text into a numpy array\n",
        "      target = np.array([target for source, target in pairs])  # extract target text into a numpy array\n",
        "\n",
        "\n",
        "    return source, target"
      ],
      "metadata": {
        "id": "x83slhi6Mn0-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src_sentences, tgt_sentences = load_data(file_path)\n",
        "print(\"Original Sentence:\",src_sentences[42])\n",
        "print(\"Translated Sentence:\",tgt_sentences[42])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LC_V-0-aMs-p",
        "outputId": "54360d7d-036f-4ac6-92b2-d717f495131e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Sentence: I know.\n",
            "Translated Sentence: Yo lo sé.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re, itertools\n",
        "from collections import Counter\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "7ATcllNjMwfc"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import unicodedata\n",
        "\n",
        "def unicode_to_ascii(s):\n",
        "    normalized = unicodedata.normalize('NFD', s)\n",
        "    return ''.join(c for c in normalized if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "def preprocess_text(text):\n",
        "  text = unicode_to_ascii(text.lower().strip())\n",
        "  text = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", text)\n",
        "  text = re.sub(r\"([?.!,¿])\", r\" \\1 \", text)\n",
        "  text = re.sub(r'[\" \"]+', \" \", text)\n",
        "  text = text.rstrip().strip()\n",
        "  text = '<sos> ' + text + ' <eos>'\n",
        "\n",
        "  return text"
      ],
      "metadata": {
        "id": "tkT4-HbgM2xd"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Original sentence:',src_sentences[42])\n",
        "prc_src_sentences = [preprocess_text(w) for w in src_sentences]\n",
        "prc_tgt_sentences = [preprocess_text(w) for w in tgt_sentences]\n",
        "print('Preprocessed sentence:',prc_src_sentences[42])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJ9qH_jWM9Di",
        "outputId": "f724e319-ea66-4ce6-ddd9-5c54d795781d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original sentence: I know.\n",
            "Preprocessed sentence: <sos> i know . <eos>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(sentences):\n",
        "    lang_tokenizer = Tokenizer( filters='')\n",
        "    lang_tokenizer.fit_on_texts(sentences)\n",
        "    sequences = lang_tokenizer.texts_to_sequences(sentences)\n",
        "    max_length = max(len(s) for s in sequences)\n",
        "    sequences = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')\n",
        "\n",
        "    return sequences, lang_tokenizer, max_length"
      ],
      "metadata": {
        "id": "7HXORTYjNDoh"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_sequences(path, size=None):\n",
        "    src_sentences, tgt_sentences = load_data(path)\n",
        "    src_sentences = [preprocess_text(w) for w in src_sentences]\n",
        "    tgt_sentences = [preprocess_text(w) for w in tgt_sentences]\n",
        "\n",
        "    src_sequences,src_lang_tokenizer,max_length_src = tokenize(src_sentences)\n",
        "    tgt_sequences,tgt_lang_tokenizer,max_length_trg = tokenize(tgt_sentences)\n",
        "\n",
        "    return src_sequences, tgt_sequences, src_lang_tokenizer, tgt_lang_tokenizer, max_length_src, max_length_trg\n"
      ],
      "metadata": {
        "id": "4sh5-kQONPCC"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src_sequences, tgt_sequences, src_lang_tokenizer, tgt_lang_tokenizer, max_length_src, max_length_trg = load_sequences(file_path)"
      ],
      "metadata": {
        "id": "7ww_nlUnNTRw"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# getting the size of the input and output vocabularies.\n",
        "src_vocab_size = len(src_lang_tokenizer.word_index)+1\n",
        "tgt_vocab_size = len(tgt_lang_tokenizer.word_index)+1\n",
        "print(src_vocab_size)\n",
        "print(tgt_vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lj6eU8-1NdzN",
        "outputId": "0b7e22be-c22d-4226-c1f8-fe6cbfa93f01"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12934\n",
            "24794\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "source_sequences_train,source_sequences_val,tgt_sequences_train,tgt_sequences_val = train_test_split(src_sequences,\n",
        "                                                                                                     tgt_sequences,\n",
        "                                                                                                     shuffle=False,\n",
        "                                                                                                     test_size=0.2)\n",
        "print(len(source_sequences_train),len(source_sequences_val),len(tgt_sequences_train),len(tgt_sequences_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COtrb8AgNhcE",
        "outputId": "64206a51-e2e6-45fa-e5d6-44e212d0de9e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "95171 23793 95171 23793\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining hyperparameters\n",
        "buffer_size=len(source_sequences_train)\n",
        "val_buffer_size = len(source_sequences_val)\n",
        "BATCH_SIZE = 64\n",
        "embedding_dim = 128\n",
        "units = 1024\n",
        "steps_per_epoch = buffer_size//BATCH_SIZE\n",
        "val_steps_per_epoch = val_buffer_size//BATCH_SIZE"
      ],
      "metadata": {
        "id": "fBUcAdcVNrv2"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((source_sequences_train, tgt_sequences_train))\n",
        "\n",
        "train_dataset = train_dataset.shuffle(buffer_size=buffer_size).batch(BATCH_SIZE)\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((source_sequences_val, tgt_sequences_val))\n",
        "\n",
        "val_dataset = val_dataset.batch(BATCH_SIZE)\n",
        "\n",
        "example_input_batch, example_target_batch = next(iter(train_dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMLenBy8NsAM",
        "outputId": "d30ca07b-57cf-4483-a972-e4dcddee8a97"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 51]), TensorShape([64, 53]))"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, vocab_size, emb_dim, enc_units, batch_sz):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.enc_units = enc_units\n",
        "        self.batch_sz = batch_sz\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, emb_dim,mask_zero=True)\n",
        "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                       return_sequences=True,\n",
        "                                       return_state=True,\n",
        "                                       recurrent_initializer='glorot_uniform')\n",
        "\n",
        "    def call(self, x, hidden):\n",
        "        x = self.embedding(x)\n",
        "        output, state = self.gru(x, initial_state = hidden)\n",
        "        return output, state\n",
        "\n",
        "    def initialize_hidden_state(self):\n",
        "        return tf.zeros((self.batch_sz, self.enc_units))\n",
        "\n",
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units) # fully-connected dense layer-1\n",
        "    self.W2 = tf.keras.layers.Dense(units) # fully-connected dense layer-2\n",
        "    self.V = tf.keras.layers.Dense(1) # fully-connected dense layer-3\n",
        "\n",
        "  def call(self, query, values):\n",
        "\n",
        "    query_with_time_axis = tf.expand_dims(query, 1)\n",
        "    score = self.V(tf.nn.tanh(self.W1(query_with_time_axis) + self.W2(values)))\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights\n",
        "\n",
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, emb_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, emb_dim)\n",
        "\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "    x = self.embedding(x)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "    output, state = self.gru(x)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "    x = self.fc(output)\n",
        "    return x, state , attention_weights"
      ],
      "metadata": {
        "id": "qjOg2sOnOG5B"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder(src_vocab_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))\n",
        "\n",
        "attention_layer = BahdanauAttention(20)\n",
        "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "\n",
        "print(\"Attention result shape (context vector): (batch size, units) {}\".format(attention_result.shape))\n",
        "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))\n",
        "\n",
        "decoder = Decoder(tgt_vocab_size, embedding_dim, units, BATCH_SIZE)\n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
        "                                      sample_hidden, sample_output)\n",
        "\n",
        "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmPEyPfAOKJz",
        "outputId": "ad35be50-b70a-4cc9-9906-72382271e179"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (64, 51, 1024)\n",
            "Encoder Hidden state shape: (batch size, units) (64, 1024)\n",
            "Attention result shape (context vector): (batch size, units) (64, 1024)\n",
            "Attention weights shape: (batch_size, sequence_length, 1) (64, 51, 1)\n",
            "Decoder output shape: (batch_size, vocab size) (64, 24794)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)\n",
        "\n",
        "checkpoint_dir = './training_checkpoints/'+TRANSLATION_PATH\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "metadata": {
        "id": "FKMnUaN7OkDq"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    dec_input = tf.expand_dims([tgt_lang_tokenizer.word_index['<sos>']] * BATCH_SIZE, 1)\n",
        "\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ],
      "metadata": {
        "id": "mu1SbVYsOnNh"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def val_step(inp, targ, enc_hidden):\n",
        "    loss = 0\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input =  tf.expand_dims([tgt_lang_tokenizer.word_index['<sos>']] * BATCH_SIZE, 1)\n",
        "\n",
        "    for t in range(1, targ.shape[1]):\n",
        "        predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "        loss += loss_function(targ[:, t], predictions)\n",
        "        dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "    batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "    return batch_loss"
      ],
      "metadata": {
        "id": "BN_FpxnTOnOw"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def train_and_validate(train_dataset, val_dataset, EPOCHS):\n",
        "    for epoch in range(EPOCHS):\n",
        "        start = time.time()\n",
        "\n",
        "        #Step1:\n",
        "        enc_hidden = encoder.initialize_hidden_state()\n",
        "        total_train_loss = 0\n",
        "        total_val_loss = 0\n",
        "        for (batch, (inp, targ)) in enumerate(train_dataset.take(steps_per_epoch)):\n",
        "            batch_loss = train_step(inp, targ, enc_hidden)\n",
        "            total_train_loss += batch_loss\n",
        "\n",
        "            if batch % 100 == 0:\n",
        "                print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                            batch,\n",
        "                                                            batch_loss.numpy()))\n",
        "\n",
        "        for (batch, (inp, targ)) in enumerate(val_dataset.take(val_steps_per_epoch)):\n",
        "            val_batch_loss = val_step(inp, targ, enc_hidden)\n",
        "            total_val_loss += val_batch_loss\n",
        "\n",
        "        if (epoch + 1) % 2 == 0:\n",
        "            checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "        print('Total training loss is {:.4f}'.format(total_train_loss / steps_per_epoch))\n",
        "        print('Total validation loss is {:.4f}'.format( total_val_loss / val_steps_per_epoch))\n",
        "        print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "metadata": {
        "id": "QRN4jSoROnTu"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########       TRAIN        ##########\n",
        "train_and_validate(train_dataset, val_dataset, EPOCHS)"
      ],
      "metadata": {
        "id": "HZuVcDWXOxlB"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create a folder in the root directory\n",
        "!mkdir -p \"/content/drive/My Drive/LiveConv/SpaEng2\"\n",
        "!mkdir -p \"/content/drive/My Drive/LiveConv/EngSpa2\"\n",
        "\n",
        "if TRANSLATION_PATH == 'spaeng':\n",
        "  !cp -rf \"/content/training_checkpoints/spaeng/\" \"/content/drive/My Drive/LiveConv/SpaEng2/\"\n",
        "else:\n",
        "  !cp -rf \"/content/training_checkpoints/engspa/\" \"/content/drive/My Drive/LiveConv/EngSpa2/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5w3HS0PPt0dN",
        "outputId": "915bd7c1-0856-4055-d223-261b1c049e69"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "  fig = plt.figure(figsize=(10, 10))\n",
        "  ax = fig.add_subplot(1, 1, 1)\n",
        "  ax.matshow(attention, cmap='viridis')\n",
        "  fontdict = {'fontsize': 14}\n",
        "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "MYi7AVrAxFDH"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(sentence):\n",
        "    attention_plot = np.zeros((max_length_trg, max_length_src))\n",
        "    sentence = preprocess_text(sentence)\n",
        "    inputs = [src_lang_tokenizer.word_index[i] for i in sentence.split(' ')]\n",
        "    inputs = pad_sequences([inputs],\n",
        "                          maxlen=max_length_src,\n",
        "                          padding='post')\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "    result = ''\n",
        "    hidden = [tf.zeros((1, units))]\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([tgt_lang_tokenizer.word_index['<sos>']], 0)\n",
        "\n",
        "    for t in range(max_length_trg):\n",
        "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                         dec_hidden,\n",
        "                                                         enc_out)\n",
        "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "        attention_plot[t] = attention_weights.numpy()\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "        result += tgt_lang_tokenizer.index_word[predicted_id] + ' '\n",
        "\n",
        "        if tgt_lang_tokenizer.index_word[predicted_id] == '<eos>':\n",
        "            return result, sentence, attention_plot\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return result, sentence, attention_plot"
      ],
      "metadata": {
        "id": "lDLU5cmDxImJ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(sentence):\n",
        "  result, sentence, attention_plot = evaluate(sentence)\n",
        "\n",
        "  print()\n",
        "  print('Input:', sentence)\n",
        "  print('Predicted Translation:', result)\n",
        "\n",
        "  attention_plot = attention_plot[:len(result.split(' ')),\n",
        "                                 :len(sentence.split(' '))]\n",
        "  plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
      ],
      "metadata": {
        "id": "tFOEaz0xxNNV"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "if TRANSLATION_PATH == 'engspa':\n",
        "  checkpoint.restore(tf.train.latest_checkpoint('/content/drive/My Drive/LiveConv/EngSpa2/))\n",
        "else:\n",
        "  checkpoint.restore(tf.train.latest_checkpoint('/content/drive/My Drive/LiveConv/SpaEng2/'))"
      ],
      "metadata": {
        "id": "cFS4-IlMxRiD"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate(u'are you still at home ?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBmRUSpuxVWE",
        "outputId": "795d10fc-0668-49eb-fc60-938f29b9ffa4"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: <sos> are you still at home ? <eos>\n",
            "Predicted Translation: ¿ todavia esta en casa ? <eos> \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translate(u'It is very cold today .')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5iFf0Mj5xgLz",
        "outputId": "15fe8650-c07c-4fc2-faad-3f5ff49fa433"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: <sos> it is very cold today . <eos>\n",
            "Predicted Translation: hoy hace mucho frio . <eos> \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Chrome Audio and getting ready for Speech to Text (using SpeechRecognitionModel)"
      ],
      "metadata": {
        "id": "yPpoEi_ByEoG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "HGCsz5uswm6x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops\n",
        "!pip install ffmpeg-python\n",
        "!pip install huggingsound\n",
        "!pip install pydub\n",
        "!pip install -U openai-whisper\n",
        "\n",
        "from IPython.display import HTML, Audio\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "from scipy.io.wavfile import read as wav_read\n",
        "import io\n",
        "import ffmpeg\n",
        "import pydub\n",
        "import whisper\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "model = whisper.load_model(\"base\")\n"
      ],
      "metadata": {
        "id": "R-2Y6LlTWHGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AUDIO_HTML = \"\"\"\n",
        "<script>\n",
        "var my_div = document.createElement(\"DIV\");\n",
        "var my_p = document.createElement(\"P\");\n",
        "var my_btn = document.createElement(\"BUTTON\");\n",
        "var t = document.createTextNode(\"Press to start recording\");\n",
        "\n",
        "my_btn.appendChild(t);\n",
        "//my_p.appendChild(my_btn);\n",
        "my_div.appendChild(my_btn);\n",
        "document.body.appendChild(my_div);\n",
        "\n",
        "var base64data = 0;\n",
        "var reader;\n",
        "var recorder, gumStream;\n",
        "var recordButton = my_btn;\n",
        "\n",
        "var handleSuccess = function(stream) {\n",
        "  gumStream = stream;\n",
        "  var options = {\n",
        "    //bitsPerSecond: 8000, //chrome seems to ignore, always 48k\n",
        "    mimeType : 'audio/webm;codecs=opus'\n",
        "    //mimeType : 'audio/webm;codecs=pcm'\n",
        "  };\n",
        "  //recorder = new MediaRecorder(stream, options);\n",
        "  recorder = new MediaRecorder(stream);\n",
        "  recorder.ondataavailable = function(e) {\n",
        "    var url = URL.createObjectURL(e.data);\n",
        "    var preview = document.createElement('audio');\n",
        "    preview.controls = true;\n",
        "    preview.src = url;\n",
        "    document.body.appendChild(preview);\n",
        "\n",
        "    reader = new FileReader();\n",
        "    reader.readAsDataURL(e.data);\n",
        "    reader.onloadend = function() {\n",
        "      base64data = reader.result;\n",
        "      //console.log(\"Inside FileReader:\" + base64data);\n",
        "    }\n",
        "  };\n",
        "  recorder.start();\n",
        "  };\n",
        "\n",
        "recordButton.innerText = \"Recording... press to stop\";\n",
        "\n",
        "navigator.mediaDevices.getUserMedia({audio: true}).then(handleSuccess);\n",
        "\n",
        "\n",
        "function toggleRecording() {\n",
        "  if (recorder && recorder.state == \"recording\") {\n",
        "      recorder.stop();\n",
        "      gumStream.getAudioTracks()[0].stop();\n",
        "      recordButton.innerText = \"Saving the recording... pls wait!\"\n",
        "  }\n",
        "}\n",
        "\n",
        "// https://stackoverflow.com/a/951057\n",
        "function sleep(ms) {\n",
        "  return new Promise(resolve => setTimeout(resolve, ms));\n",
        "}\n",
        "\n",
        "var data = new Promise(resolve=>{\n",
        "//recordButton.addEventListener(\"click\", toggleRecording);\n",
        "recordButton.onclick = ()=>{\n",
        "toggleRecording()\n",
        "\n",
        "sleep(2000).then(() => {\n",
        "  // wait 2000ms for the data to be available...\n",
        "  // ideally this should use something like await...\n",
        "  //console.log(\"Inside data:\" + base64data)\n",
        "  resolve(base64data.toString())\n",
        "\n",
        "});\n",
        "\n",
        "}\n",
        "});\n",
        "\n",
        "</script>\n",
        "\"\"\"\n",
        "\n",
        "def get_audio():\n",
        "  display(HTML(AUDIO_HTML))\n",
        "  data = eval_js(\"data\")\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "\n",
        "  process = (ffmpeg\n",
        "    .input('pipe:0')\n",
        "    .output('pipe:1', format='wav')\n",
        "    .run_async(pipe_stdin=True, pipe_stdout=True, pipe_stderr=True, quiet=True, overwrite_output=True)\n",
        "  )\n",
        "  output, err = process.communicate(input=binary)\n",
        "\n",
        "  riff_chunk_size = len(output) - 8\n",
        "  # Break up the chunk size into four bytes, held in b.\n",
        "  q = riff_chunk_size\n",
        "  b = []\n",
        "  for i in range(4):\n",
        "      q, r = divmod(q, 256)\n",
        "      b.append(r)\n",
        "\n",
        "  # Replace bytes 4:8 in proc.stdout with the actual size of the RIFF chunk.\n",
        "  riff = output[:4] + bytes(b) + output[8:]\n",
        "\n",
        "  sr, audio = wav_read(io.BytesIO(riff))\n",
        "\n",
        "  return audio, sr\n",
        "\n",
        "\n",
        "def write(f, sr, x, normalized=False):\n",
        "    \"\"\"numpy array to MP3\"\"\"\n",
        "    channels = 2 if (x.ndim == 2 and x.shape[1] == 2) else 1\n",
        "    if normalized:  # normalized array - each item should be a float in [-1, 1)\n",
        "        y = np.int16(x * 2 ** 15)\n",
        "    else:\n",
        "        y = np.int16(x)\n",
        "    song = pydub.AudioSegment(y.tobytes(), frame_rate=sr, sample_width=2, channels=channels)\n",
        "    song.export(f, format=\"mp3\", bitrate=\"320k\")\n"
      ],
      "metadata": {
        "id": "EC1123uXWPGZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio, sr = get_audio()\n",
        "write('/content/out2.mp3', sr, audio)\n",
        "\n",
        "# load audio and pad/trim it to fit 30 seconds\n",
        "audio = whisper.load_audio('/content/out2.mp3')\n",
        "audio = whisper.pad_or_trim(audio)\n",
        "\n",
        "# make log-Mel spectrogram and move to the same device as the model\n",
        "mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
        "\n",
        "# detect the spoken language\n",
        "_, probs = model.detect_language(mel)\n",
        "input_language = max(probs, key=probs.get)\n",
        "print(f\"Detected language: {input_language}\")\n",
        "\n",
        "# decode the audio\n",
        "options = whisper.DecodingOptions()\n",
        "result = whisper.decode(model, mel, options)\n",
        "\n",
        "# print the recognized text\n",
        "print(result.text)\n",
        "\n",
        "if input_language == 'en':\n",
        "  checkpoint.restore(tf.train.latest_checkpoint('/content/drive/My Drive/LiveConv/EngSpa2/'))\n",
        "  translate(sentence)\n",
        "else\n",
        "  checkpoint.restore(tf.train.latest_checkpoint('/content/drive/My Drive/LiveConv/SpaEng2/'))\n",
        "  translate(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "id": "NV_i3Ic8tNeG",
        "outputId": "7e9a01e9-9820-4527-b243-0042720ca380"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai-whisper in /usr/local/lib/python3.10/dist-packages (20231117)\n",
            "Requirement already satisfied: triton<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (2.1.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (0.58.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (1.23.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (1.12.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (4.66.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (10.1.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (0.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton<3,>=2.0.0->openai-whisper) (3.13.1)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper) (0.41.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2024.2.2)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<script>\n",
              "var my_div = document.createElement(\"DIV\");\n",
              "var my_p = document.createElement(\"P\");\n",
              "var my_btn = document.createElement(\"BUTTON\");\n",
              "var t = document.createTextNode(\"Press to start recording\");\n",
              "\n",
              "my_btn.appendChild(t);\n",
              "//my_p.appendChild(my_btn);\n",
              "my_div.appendChild(my_btn);\n",
              "document.body.appendChild(my_div);\n",
              "\n",
              "var base64data = 0;\n",
              "var reader;\n",
              "var recorder, gumStream;\n",
              "var recordButton = my_btn;\n",
              "\n",
              "var handleSuccess = function(stream) {\n",
              "  gumStream = stream;\n",
              "  var options = {\n",
              "    //bitsPerSecond: 8000, //chrome seems to ignore, always 48k\n",
              "    mimeType : 'audio/webm;codecs=opus'\n",
              "    //mimeType : 'audio/webm;codecs=pcm'\n",
              "  };\n",
              "  //recorder = new MediaRecorder(stream, options);\n",
              "  recorder = new MediaRecorder(stream);\n",
              "  recorder.ondataavailable = function(e) {\n",
              "    var url = URL.createObjectURL(e.data);\n",
              "    var preview = document.createElement('audio');\n",
              "    preview.controls = true;\n",
              "    preview.src = url;\n",
              "    document.body.appendChild(preview);\n",
              "\n",
              "    reader = new FileReader();\n",
              "    reader.readAsDataURL(e.data);\n",
              "    reader.onloadend = function() {\n",
              "      base64data = reader.result;\n",
              "      //console.log(\"Inside FileReader:\" + base64data);\n",
              "    }\n",
              "  };\n",
              "  recorder.start();\n",
              "  };\n",
              "\n",
              "recordButton.innerText = \"Recording... press to stop\";\n",
              "\n",
              "navigator.mediaDevices.getUserMedia({audio: true}).then(handleSuccess);\n",
              "\n",
              "\n",
              "function toggleRecording() {\n",
              "  if (recorder && recorder.state == \"recording\") {\n",
              "      recorder.stop();\n",
              "      gumStream.getAudioTracks()[0].stop();\n",
              "      recordButton.innerText = \"Saving the recording... pls wait!\"\n",
              "  }\n",
              "}\n",
              "\n",
              "// https://stackoverflow.com/a/951057\n",
              "function sleep(ms) {\n",
              "  return new Promise(resolve => setTimeout(resolve, ms));\n",
              "}\n",
              "\n",
              "var data = new Promise(resolve=>{\n",
              "//recordButton.addEventListener(\"click\", toggleRecording);\n",
              "recordButton.onclick = ()=>{\n",
              "toggleRecording()\n",
              "\n",
              "sleep(2000).then(() => {\n",
              "  // wait 2000ms for the data to be available...\n",
              "  // ideally this should use something like await...\n",
              "  //console.log(\"Inside data:\" + base64data)\n",
              "  resolve(base64data.toString())\n",
              "\n",
              "});\n",
              "\n",
              "}\n",
              "});\n",
              "\n",
              "</script>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected language: en\n",
            "I have a sister.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}